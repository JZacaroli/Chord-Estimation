{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as nnF\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, glob, csv\n",
    "import numpy as np\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many time steps to input with each datapoint. (=WIDTH OF RECURRENT NETWORK)\n",
    "num_of_time_steps_in_input = 50\n",
    "num_of_input_chromas = 24\n",
    "\n",
    "# Chord vocabulary is defined in this dictionary.\n",
    "#Major/Minor vocab\n",
    "#chord_dict = {'N':0, 'X':0, 'C:maj':1, 'C:min':2, 'Db:maj':3, 'C#:maj':3, 'Db:min':4, 'C#:min':4, 'D:maj':5, 'D:min':6,\n",
    "#                      'Eb:maj':7, 'D#:maj':7, 'Eb:min':8, 'D#:min':8, 'E:maj':9, 'Fb:maj':9, 'E:min':10, 'F:maj':11, 'F:min':12,\n",
    "#                      'Gb:maj':13, 'F#:maj':13, 'Gb:min':14, 'F#:min':14, 'G:maj':15, 'G:min':16,\n",
    "#                      'Ab:maj':17, 'G#:maj':17, 'Ab:min':18, 'G#:min':18, 'A:maj':19, 'A:min':20,\n",
    "#                      'Bb:maj':21, 'A#:maj':21, 'Bb:min':22, 'A#:min':22, 'B:maj':23, 'Cb:maj':23, 'B:min':24, 'Cb:min':24}\n",
    "#chord_annotations_directory = 'Data/ChordAnnotations/McGill-GroundTruth/'\n",
    "#chord_dict['Cb:maj']\n",
    "\n",
    "#Major(7)/Minor(7) vocab.\n",
    "#chord_dict = {'N':0, 'X':0, 'C:maj':1, 'C:min':2, 'Db:maj':3, 'C#:maj':3, 'Db:min':4, 'C#:min':4, 'D:maj':5, 'D:min':6,\n",
    "#                      'Eb:maj':7, 'D#:maj':7, 'Eb:min':8, 'D#:min':8, 'E:maj':9, 'Fb:maj':9, 'E:min':10, 'F:maj':11, 'F:min':12,\n",
    "#                      'Gb:maj':13, 'F#:maj':13, 'Gb:min':14, 'F#:min':14, 'G:maj':15, 'G:min':16,\n",
    "#                      'Ab:maj':17, 'G#:maj':17, 'Ab:min':18, 'G#:min':18, 'A:maj':19, 'A:min':20,\n",
    "#                      'Bb:maj':21, 'A#:maj':21, 'Bb:min':22, 'A#:min':22, 'B:maj':23, 'Cb:maj':23, 'B:min':24, 'Cb:min':24,\n",
    "#                      'C:maj7':25, 'C:min7':26, 'C:7':27, 'Db:maj7':28, 'C#:maj7':28, 'Db:min7':29, 'C#:min7':29,\n",
    "#                      'Db:7':30, 'C#:7':30, 'D:maj7':31, 'D:min7':32, 'D:7':33, 'Eb:maj7':34, 'D#:maj7':34,\n",
    "#                      'Eb:min7':35, 'D#:min7':35, 'Eb:7':36, 'D#:7':36, 'E:maj7':37, 'Fb:maj7':37, 'E:min7':38,\n",
    "#                      'E:7':39, 'F:maj7':40, 'F:min7':41, 'F:7':42, 'F#:maj7':43, 'Gb:maj7':43, 'F#:min7':44, 'Gb:min7':44,\n",
    "#                      'F#:7':45, 'Gb:7':45, 'G:maj7':46, 'G:min7':47, 'G:7':48, 'Ab:maj7':49, 'G#:maj7':49,\n",
    "#                      'Ab:min7':50, 'G#:min7':50, 'Ab:7':51, 'G#:7':51, 'A:maj7':52, 'A:min7':53, 'A:7':54,\n",
    "#                      'Bb:maj7':55, 'A#:maj7':55, 'Bb:min7':56, 'A#:min7':56, 'Bb:7':57, 'A#:7':57, 'B:maj7':58,\n",
    "#                      'Cb:maj7':59, 'B:min7':59, 'Cb:min7':59, 'B:7':60, 'Cb:7':60}\n",
    "#chord_annotations_directory = 'Data/ChordAnnotations_majmin7/McGill-GroundTruth_majmin7/'\n",
    "\n",
    "#Major/Minor with inversions vocab.\n",
    "chord_dict = {'N':0, 'X':0, 'C:maj':1, 'C:min':2, 'Db:maj':3, 'C#:maj':3, 'Db:min':4, 'C#:min':4, 'D:maj':5, 'D:min':6,\n",
    "              'Eb:maj':7, 'D#:maj':7, 'Eb:min':8, 'D#:min':8, 'E:maj':9, 'Fb:maj':9, 'E:min':10, 'F:maj':11, 'F:min':12,\n",
    "              'Gb:maj':13, 'F#:maj':13, 'Gb:min':14, 'F#:min':14, 'G:maj':15, 'G:min':16,\n",
    "              'Ab:maj':17, 'G#:maj':17, 'Ab:min':18, 'G#:min':18, 'A:maj':19, 'A:min':20,\n",
    "              'Bb:maj':21, 'A#:maj':21, 'Bb:min':22, 'A#:min':22, 'B:maj':23, 'Cb:maj':23, 'B:min':24, 'Cb:min':24,\n",
    "              'C:maj/3':25, 'C:min/b3':26, 'C:maj/5':27, 'C:min/5':28, 'Db:maj/3':29, 'C#:maj/3':29, 'Db:min/b3':30,\n",
    "              'C#:min/b3':30, 'Db:maj/5':31, 'C#:maj/5':31, 'Db:min/5':32, 'C#:min/5':32, 'D:maj/3':33, 'D:min/b3':34,\n",
    "              'D:maj/5':35, 'D:min/5':36, 'Eb:maj/3':37, 'D#:maj/3':37, 'Eb:min/b3':38, 'D#:min/b3':38,\n",
    "              'Eb:maj/5':39, 'D#:maj/5':39, 'Eb:min/5':40, 'D#:min/5':40, 'E:maj/3':41, 'Fb:maj/3':41, 'E:min/b3':42,\n",
    "              'E:maj/5':43, 'Fb:maj/5':43, 'E:min/5':44, 'F:maj/3':45, 'F:min/b3':46, 'F:maj/5':47, 'F:min/5':48,\n",
    "              'Gb:maj/3':49, 'F#:maj/3':49, 'Gb:min/b3':50, 'F#:min/b3':50, 'Gb:maj/5':51, 'F#:maj/5':51,\n",
    "              'Gb:min/5':52, 'F#:min/5':52, 'G:maj/3':53, 'G:min/b3':54, 'G:maj/5':55, 'G:min/5':56, 'Ab:maj/3':57,\n",
    "              'G#:maj/3':57, 'Ab:min/b3':58, 'G#:min/b3':58, 'Ab:maj/5':59, 'G#:maj/5':59, 'Ab:min/5':60, 'G#:min/5':60,\n",
    "              'A:maj/3':61, 'A:min/b3':62, 'A:maj/5':63, 'A:min/5':64, 'Bb:maj/3':65, 'A#:maj/3':65, 'Bb:min/b3':66,\n",
    "              'A#:min/b3':66, 'Bb:maj/5':67, 'A#:maj/5':67, 'Bb:min/5':68, 'A#:min/5':68, 'B:maj/3':69, 'Cb:maj/3':69,\n",
    "              'B:min/b3':70, 'Cb:min/b3':70, 'B:maj/5':71, 'Cb:maj/5':71, 'B:min/5':72, 'Cb:min/5':72}\n",
    "chord_annotations_directory = 'Data/ChordAnnotations_majmininv/McGill-GroundTruth_majmininv/'\n",
    "\n",
    "\n",
    "\n",
    "num_of_chord_classes = max(chord_dict.values())+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainAndTestSets(train_set_size, test_set_size, csv_file_numbers, time=1):\n",
    "\n",
    "    train_set = []\n",
    "    train_samples_added = 0\n",
    "    while train_samples_added < train_set_size:\n",
    "        #Pick a random index in the csv_file_numbers\n",
    "        rand_index = randint(0,len(csv_file_numbers)-1)\n",
    "        #Add the file number at that index to the train set\n",
    "        train_set += [csv_file_numbers[rand_index]]\n",
    "        #Remove the file number at that index from the list of file numbers\n",
    "        csv_file_numbers.pop(rand_index)\n",
    "        \n",
    "        train_samples_added += 1\n",
    "\n",
    "    test_set = []\n",
    "    test_samples_added = 0\n",
    "    while test_samples_added < test_set_size:\n",
    "        #Pick a random index in the csv_file_numbers\n",
    "        rand_index = randint(0,len(csv_file_numbers)-1)\n",
    "        #Add the file number at that index to the train set\n",
    "        test_set += [csv_file_numbers[rand_index]]\n",
    "        #Remove the file number at that index from the list of file numbers\n",
    "        csv_file_numbers.pop(rand_index)\n",
    "        \n",
    "        test_samples_added += 1\n",
    "    \n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChordDataset(Dataset):\n",
    "    def __init__(self, csv_file_numbers=-1, num_of_datapoints_per_file=-1, file_to_load_from=-1):\n",
    "        \"\"\"Creates a dataset from a list of csv file numbers.\n",
    "        Creates a variable number of data points per file. Takes datapoints in neighbouring blocks.\n",
    "        There are two types of csv file:\n",
    "        1) A set of chromagrams for each time step\n",
    "        2) A chord symbol for each time step\n",
    "        Both of these need to be accessed to find a datapoint.\"\"\"\n",
    "        if file_to_load_from != -1:\n",
    "            print('Loading from ' + file_to_load_from)\n",
    "            self.samples = np.load(file_to_load_from)\n",
    "            self.length = self.samples.shape[0]\n",
    "        else:\n",
    "            self.num_of_datapoints_per_file = num_of_datapoints_per_file\n",
    "            self.length = self.num_of_datapoints_per_file*len(csv_file_numbers)\n",
    "\n",
    "            #Samples will be a numpy array of dimensions: [num_of_samples, num_of_time_steps, 25 (24 chromas + 1 label)]\n",
    "            self.samples = np.zeros((self.length, num_of_time_steps_in_input, num_of_input_chromas+1))\n",
    "\n",
    "            #Go through each of the csv files and add them to the np array of samples.\n",
    "            for num_of_file_used, csv_file_number in enumerate(csv_file_numbers):\n",
    "                for i in range(num_of_datapoints_per_file):\n",
    "                    sample_number = num_of_file_used*self.num_of_datapoints_per_file + i\n",
    "                    sample = np.zeros((num_of_time_steps_in_input, num_of_input_chromas+1))\n",
    "\n",
    "                    #Work out how long this song is. (row_count)\n",
    "                    csv_chroma_filename = 'Data/Chromagrams/McGill-Chromagrams/'+csv_file_number+'_bothchroma.csv'\n",
    "                    with open(csv_chroma_filename, 'r') as chroma_file:\n",
    "                        rdr = csv.reader(chroma_file)\n",
    "                        row_count = sum(1 for row in rdr)\n",
    "\n",
    "                    #A lot of the time the first 500 or so rows are no chord so take these points out.\n",
    "                    random_starting_row = randint(500, row_count-num_of_time_steps_in_input)\n",
    "\n",
    "                    #Add the chroma values for this sample\n",
    "                    with open(csv_chroma_filename, 'r') as chroma_file:\n",
    "                        rdr = csv.reader(chroma_file)\n",
    "\n",
    "                        desiredrows=[row for idx, row in enumerate(rdr) if idx in range(random_starting_row,random_starting_row+num_of_time_steps_in_input)]\n",
    "                        for idx, row in enumerate(desiredrows):\n",
    "                            #row is ['','{TimeInstant}','{ChordSymbol}']\n",
    "                            #Add the chroma values to the sample at the desired time index\n",
    "                            sample[idx, 0:num_of_input_chromas] = row[2:]\n",
    "\n",
    "                    #Add the chord label for this sample\n",
    "                    if num_of_chord_classes == 25:\n",
    "                        csv_chord_filename = chord_annotations_directory+csv_file_number+'_chords.csv'#used for T=50\n",
    "                    else:\n",
    "                        csv_chord_filename = chord_annotations_directory+csv_file_number+'.csv' #Used for T!=50\n",
    "                    \n",
    "                    with open(csv_chord_filename, 'r') as chord_file:\n",
    "                        rdr = csv.reader(chord_file)\n",
    "                        desiredrows=[row for idx, row in enumerate(rdr) if idx in range(random_starting_row,random_starting_row+num_of_time_steps_in_input)]\n",
    "                        for idx, row in enumerate(desiredrows):\n",
    "                            #row is ['{TimeInstant}\\t{ChordSymbol}']\n",
    "                            #Add the chord label to the sample at the desired time index\n",
    "                            sample[idx, num_of_input_chromas] = chord_dict[row[0].split('\\t')[1]]\n",
    "\n",
    "                    self.samples[sample_number, :, :] = sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample_ = self.samples[index,:,:]\n",
    "        sample = {\n",
    "            'chromas': sample_[:, 0:num_of_input_chromas], #Chromas are up to the num_of_chromas\n",
    "            'labels': sample_[:, num_of_input_chromas] #Labels are the one after\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "    def saveToFile(self, fileName):\n",
    "        np.save(fileName, self.samples)\n",
    "        \n",
    "#Create a certain number of datapoints from each file\n",
    "num_of_files_in_dataset = 890\n",
    "num_of_datapoints_per_file = 20\n",
    "num_of_datapoints = num_of_files_in_dataset*num_of_datapoints_per_file\n",
    "\n",
    "# The total number of files\n",
    "#dataset_size = 890\n",
    "num_in_train_set = 0.95*num_of_files_in_dataset//1\n",
    "num_in_test_set = 0.05*num_of_files_in_dataset//1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_numbers = []\n",
    "for f in os.scandir('Data/ChordAnnotations/McGill-GroundTruth'):\n",
    "    if f.name.endswith('.csv'):\n",
    "        csv_file_numbers += [f.name[0:4]]\n",
    "assert(len(csv_file_numbers) == num_of_files_in_dataset)\n",
    "\n",
    "#Each set is assigned a certain number of csv file numbers which are\n",
    "#then accessed and loaded inside the ChordDataset class.\n",
    "train_set, test_set = createTrainAndTestSets(num_in_train_set, num_in_test_set, csv_file_numbers, time=1)\n",
    "train_set = ChordDataset(csv_file_numbers=train_set, num_of_datapoints_per_file=num_of_datapoints_per_file)\n",
    "test_set = ChordDataset(csv_file_numbers=test_set, num_of_datapoints_per_file=num_of_datapoints_per_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dataset to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetFileName = 'TrainSet_inputSize_' + str(num_of_time_steps_in_input) + '_chordVocab_' + str(num_of_chord_classes)\n",
    "testSetFileName = 'TestSet_inputSize_' + str(num_of_time_steps_in_input) + '_chordVocab_' + str(num_of_chord_classes)\n",
    "train_set.saveToFile(trainSetFileName)\n",
    "test_set.saveToFile(testSetFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from TrainSet_inputSize_50_chordVocab_73.npy\n",
      "Loading from TestSet_inputSize_50_chordVocab_73.npy\n"
     ]
    }
   ],
   "source": [
    "trainSetFileName = 'TrainSet_inputSize_' + str(num_of_time_steps_in_input) + '_chordVocab_' + str(num_of_chord_classes)\n",
    "testSetFileName = 'TestSet_inputSize_' + str(num_of_time_steps_in_input) + '_chordVocab_' + str(num_of_chord_classes)\n",
    "train_set = ChordDataset(file_to_load_from=trainSetFileName + '.npy')\n",
    "test_set = ChordDataset(file_to_load_from=testSetFileName + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=BATCH_SIZE\n",
    ")\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecurrentNet, self).__init__()\n",
    "        #Fully connected layer.\n",
    "        #self.fc1 = nn.Linear(num_of_time_steps_in_input*num_of_input_chromas,num_of_time_steps_in_input*num_of_input_chromas)\n",
    "        \n",
    "        #Recurrent layer.\n",
    "        #self.rnn1 = nn.RNN(input_size=24, hidden_size=25, batch_first=True)\n",
    "        \n",
    "        #Convolutional layer. Only convolve along the time dimension, keeping the time dimension intact.\n",
    "        #self.conv1 = nn.Conv2d(1,1,(1,3), padding=(0,1), stride=1, dilation=1)\n",
    "        \n",
    "        #Dilated Convolutional layer.\n",
    "        #self.conv2 = nn.Conv2d(1,1,(1,3), padding=(0,2), stride=1, dilation=2)\n",
    "        \n",
    "        #Dilated Convolutional layer.\n",
    "        #self.conv3 = nn.Conv2d(1,1,(1,3), padding=(0,3), stride=1, dilation=3)\n",
    "        \n",
    "        #Time-distributed dense layer. Convolutional kernels with as many\n",
    "        #output channels as inputs in the one input channel. And a kernel of height 24 and width 1.\n",
    "        #(inChannels, outChannels, (kernelHeight, kernelWidth))\n",
    "        #self.conv4 = nn.Conv2d(1,num_of_chord_classes,(num_of_input_chromas,1))\n",
    "        \n",
    "        #RECURRENT LAYER\n",
    "        self.num_recurrent_layers=3\n",
    "        self.bidirectional=False\n",
    "        self.lstm1 = nn.LSTM(input_size=num_of_input_chromas, hidden_size=num_of_chord_classes, batch_first=True, num_layers=self.num_recurrent_layers, dropout=0.1, bidirectional=self.bidirectional)\n",
    "        \n",
    "        #Fully connected output layers\n",
    "        #self.fc_out_1 = nn.Linear(num_of_time_steps_in_input*num_of_chord_classes, num_of_time_steps_in_input*num_of_chord_classes)\n",
    "        #self.fc_out_2 = nn.Linear(num_of_time_steps_in_input*num_of_chord_classes, num_of_time_steps_in_input*num_of_chord_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Input is of shape (BATCH_SIZE, TIME_STEPS, CHROMAS_PER_TIME_STEP)\n",
    "        current_batch_size = x.shape[0]\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #Fully connected layer\n",
    "        #x = torch.sigmoid(self.fc1(x.view(current_batch_size, num_of_time_steps_in_input*num_of_input_chromas)))\n",
    "        \n",
    "        #Convolutional layer.\n",
    "        #x = nnF.relu(self.conv1(x.view(current_batch_size, 1, num_of_input_chromas, num_of_time_steps_in_input)))\n",
    "        \n",
    "        #Dilated convolutional layer\n",
    "        #x = nnF.relu(self.conv2(x.view(current_batch_size, 1, num_of_input_chromas, num_of_time_steps_in_input)))\n",
    "        \n",
    "        #Dilated-er convolutional layer\n",
    "        #x = nnF.relu(self.conv3(x.view(current_batch_size, 1, num_of_input_chromas, num_of_time_steps_in_input)))\n",
    "        \n",
    "        #Time distributed layer.\n",
    "        #x = nnF.relu(self.conv4(x.view(current_batch_size,1,num_of_input_chromas,num_of_time_steps_in_input)))\n",
    "        \n",
    "        #Initialise hidden state with (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        if self.bidirectional:\n",
    "            h0 = torch.zeros(2*self.num_recurrent_layers, current_batch_size, num_of_chord_classes).double().to(device)\n",
    "            c0 = torch.zeros(2*self.num_recurrent_layers, current_batch_size, num_of_chord_classes).double().to(device)\n",
    "        else:\n",
    "            h0 = torch.zeros(self.num_recurrent_layers, current_batch_size, num_of_chord_classes).double().to(device)\n",
    "            c0 = torch.zeros(self.num_recurrent_layers, current_batch_size, num_of_chord_classes).double().to(device)\n",
    "\n",
    "        x = x.view(current_batch_size,num_of_time_steps_in_input,num_of_input_chromas)\n",
    "        x, hn = self.lstm1(x, (h0,c0))\n",
    "\n",
    "        #Fully connected output layers\n",
    "        #x = nnF.relu(self.fc_out_1(x.reshape(current_batch_size, num_of_time_steps_in_input*num_of_chord_classes))).view(current_batch_size, num_of_time_steps_in_input, num_of_chord_classes)\n",
    "        #x = nnF.relu(self.fc_out_2(x.reshape(current_batch_size, num_of_time_steps_in_input*num_of_chord_classes))).view(current_batch_size, num_of_time_steps_in_input, num_of_chord_classes)\n",
    "         \n",
    "        #print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RecurrentNet()\n",
    "net.double()\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "device = torch.device('cuda:3')\n",
    "cpuDevice = torch.device('cpu')\n",
    "net.to(device)\n",
    "\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    'test': []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    'test': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 2)\n",
    "    #print('y_pred_softmax shape', y_pred_softmax.shape)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 2)    \n",
    "    #print('y_pred_tags shape', y_pred_tags.shape)\n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    #print(correct_pred)\n",
    "    acc = correct_pred.sum() / correct_pred.numel()\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training ..\n",
      "Epoch 000: | Train Loss: 3.44359 | Test Loss: 3.31551 | Train Acc: 10.185| Test Acc: 9.714\n",
      "Epoch 001: | Train Loss: 3.17169 | Test Loss: 3.10640 | Train Acc: 15.792| Test Acc: 16.357\n",
      "Epoch 002: | Train Loss: 3.03376 | Test Loss: 3.01254 | Train Acc: 18.400| Test Acc: 19.214\n",
      "Epoch 003: | Train Loss: 2.96838 | Test Loss: 2.97293 | Train Acc: 19.777| Test Acc: 20.286\n",
      "Epoch 004: | Train Loss: 2.93754 | Test Loss: 2.95414 | Train Acc: 21.192| Test Acc: 22.357\n",
      "Epoch 005: | Train Loss: 2.91677 | Test Loss: 2.93401 | Train Acc: 23.045| Test Acc: 24.429\n",
      "Epoch 006: | Train Loss: 2.89774 | Test Loss: 2.91123 | Train Acc: 24.464| Test Acc: 24.571\n",
      "Epoch 007: | Train Loss: 2.87784 | Test Loss: 2.89913 | Train Acc: 25.702| Test Acc: 25.000\n",
      "Epoch 008: | Train Loss: 2.86150 | Test Loss: 2.88344 | Train Acc: 29.219| Test Acc: 29.000\n",
      "Epoch 009: | Train Loss: 2.84631 | Test Loss: 2.86860 | Train Acc: 30.974| Test Acc: 29.571\n",
      "Epoch 010: | Train Loss: 2.83690 | Test Loss: 2.86075 | Train Acc: 31.732| Test Acc: 30.571\n",
      "Epoch 011: | Train Loss: 2.83009 | Test Loss: 2.85968 | Train Acc: 32.543| Test Acc: 31.929\n",
      "Epoch 012: | Train Loss: 2.82467 | Test Loss: 2.85504 | Train Acc: 33.385| Test Acc: 32.643\n",
      "Epoch 013: | Train Loss: 2.81760 | Test Loss: 2.85104 | Train Acc: 35.200| Test Acc: 37.714\n",
      "Epoch 014: | Train Loss: 2.81239 | Test Loss: 2.84744 | Train Acc: 36.091| Test Acc: 34.571\n",
      "Epoch 015: | Train Loss: 2.80936 | Test Loss: 2.84257 | Train Acc: 38.770| Test Acc: 42.643\n",
      "Epoch 016: | Train Loss: 2.80371 | Test Loss: 2.83613 | Train Acc: 42.815| Test Acc: 49.071\n",
      "Epoch 017: | Train Loss: 2.79659 | Test Loss: 2.83601 | Train Acc: 49.898| Test Acc: 49.071\n",
      "Epoch 018: | Train Loss: 2.79344 | Test Loss: 2.82871 | Train Acc: 50.158| Test Acc: 48.286\n",
      "Epoch 019: | Train Loss: 2.78755 | Test Loss: 2.83487 | Train Acc: 50.577| Test Acc: 49.071\n",
      "Epoch 020: | Train Loss: 2.78468 | Test Loss: 2.82379 | Train Acc: 51.445| Test Acc: 50.214\n",
      "Epoch 021: | Train Loss: 2.77828 | Test Loss: 2.82275 | Train Acc: 52.536| Test Acc: 50.857\n",
      "Epoch 022: | Train Loss: 2.77608 | Test Loss: 2.82488 | Train Acc: 53.249| Test Acc: 50.857\n",
      "Epoch 023: | Train Loss: 2.77320 | Test Loss: 2.81756 | Train Acc: 53.808| Test Acc: 51.429\n",
      "Epoch 024: | Train Loss: 2.77106 | Test Loss: 2.82344 | Train Acc: 54.513| Test Acc: 52.571\n",
      "Epoch 025: | Train Loss: 2.76670 | Test Loss: 2.81459 | Train Acc: 55.113| Test Acc: 51.643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-675-795830947707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ecs7013P/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_ecs7013P/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_of_epochs = 100\n",
    "\n",
    "print('Beginning Training ..')\n",
    "\n",
    "prevNet = -1\n",
    "\n",
    "for epoch in range(num_of_epochs):\n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    \n",
    "    net.train()\n",
    "    i=0\n",
    "    for train_batch in train_loader:\n",
    "        #X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        X_train_batch = train_batch['chromas'].double().to(device)\n",
    "        #print(X_train_batch[0,:,:])\n",
    "        y_train_batch = train_batch['labels'].long().to(device)\n",
    "        #print(y_train_batch[0,:])\n",
    "        current_batch_size = X_train_batch.shape[0]\n",
    "        \n",
    "        y_train_pred = net(X_train_batch)\n",
    "        #if i==0:\n",
    "        #    chordSelections=[]\n",
    "        #    actualChords=[]\n",
    "        #    \n",
    "        #    for row in y_train_pred[0]:\n",
    "        #        chordSelections += [torch.argmax(row).item()]\n",
    "        #    for item in y_train_batch[0]:\n",
    "        #        actualChords += [item.item()]\n",
    "        #    #actualChords = list(y_train_batch[0,:].item())\n",
    "        #    print(chordSelections)\n",
    "        #    print(actualChords)\n",
    "        #    i+=1\n",
    "        \n",
    "        train_loss = cross_entropy_loss(y_train_pred.reshape(current_batch_size*num_of_time_steps_in_input,num_of_chord_classes),\n",
    "                                        y_train_batch.view(-1))\n",
    "        train_acc = batch_accuracy(y_train_pred, y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION   \n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_acc = 0\n",
    "        \n",
    "        net.eval()\n",
    "        for test_batch in test_loader:\n",
    "            #X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            X_test_batch = test_batch['chromas'].double().to(device)\n",
    "            y_test_batch = test_batch['labels'].long().to(device)\n",
    "            current_batch_size = X_test_batch.shape[0]\n",
    "            \n",
    "            y_test_pred = net(X_test_batch)\n",
    "            \n",
    "            #Print out output for a whole batch\n",
    "            #if i==12:\n",
    "            #    for j,batchitem in enumerate(y_test_pred):\n",
    "            #        chordSelections=[]\n",
    "            #        actualChords=[]\n",
    "            #\n",
    "            #        for row in y_test_pred[j]:\n",
    "            #            chordSelections += [chord_dict_reverse[torch.argmax(row).item()]]\n",
    "            #        for item in y_test_batch[j]:\n",
    "            #            actualChords += [chord_dict_reverse[item.item()]]\n",
    "            #        #actualChords = list(y_train_batch[0,:].item())\n",
    "            #        print(chordSelections)\n",
    "            #        print(actualChords)\n",
    "            #i+=1\n",
    "            \n",
    "            test_loss = cross_entropy_loss(y_test_pred.reshape(current_batch_size*num_of_time_steps_in_input,num_of_chord_classes),\n",
    "                                           y_test_batch.view(-1))\n",
    "            test_acc = batch_accuracy(y_test_pred, y_test_batch)\n",
    "            \n",
    "            test_epoch_loss += test_loss.item()\n",
    "            test_epoch_acc += test_acc.item()\n",
    "            \n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['test'].append(test_epoch_loss/len(test_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['test'].append(test_epoch_acc/len(test_loader))\n",
    "    \n",
    "    print(f'Epoch {epoch+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Test Loss: {test_epoch_loss/len(test_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Test Acc: {test_epoch_acc/len(test_loader):.3f}')\n",
    "    prevNet = net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the chopin file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First create a mapping from class number to chord\n",
    "#MajMin Chord Vocab\n",
    "#chord_dict_reverse = {0:'N', 1:'C:maj', 2:'C:min', 3:'Db:maj', 4:'Db:min', 5:'D:maj', 6:'D:min', 7:'Eb:maj',\n",
    "#                      8:'Eb:min', 9:'E:maj', 10:'E:min', 11:'F:maj', 12:'F:min', 13:'Gb:maj', 14:'Gb:min', 15:'G:maj',\n",
    "#                      16:'G:min', 17: 'Ab:maj', 18:'Ab:min', 19:'A:maj', 20:'A:min', 21:'Bb:maj', 22:'Bb:min',\n",
    "#                      23:'B:maj', 24:'B:min'}\n",
    "\n",
    "\n",
    "#MajMinInv Chord Vocab\n",
    "chord_dict_reverse = {0:'N', 1:'C:maj', 2:'C:min', 3:'Db:maj', 4:'Db:min', 5:'D:maj', 6:'D:min', 7:'Eb:maj',\n",
    "                     8:'Eb:min', 9:'E:maj', 10:'E:min', 11:'F:maj', 12:'F:min', 13:'Gb:maj', 14:'Gb:min', 15:'G:maj',\n",
    "                     16:'G:min', 17: 'Ab:maj', 18:'Ab:min', 19:'A:maj', 20:'A:min', 21:'Bb:maj', 22:'Bb:min',\n",
    "                     23:'B:maj', 24:'B:min', 25:'C:maj/3', 26:'C:min/b3', 27:'C:maj/5', 28:'C:min/5', 29:'Db:maj/3',\n",
    "                     30:'Db:min/b3', 31:'Db:maj/5', 32:'Db:min/5', 33:'D:maj/3', 34:'D:min/b3', 35:'D:maj/5',\n",
    "                     36:'D:min/5', 37:'Eb:maj/3', 38:'Eb:min/b3', 39:'Eb:maj/5', 40:'Eb:min/5', 41:'E:maj/3',\n",
    "                     42:'E:min/b3', 43:'E:maj/5', 44:'E:min/5', 45:'F:maj/3', 46:'F:min/b3', 47:'F:maj/5', 48:'F:min/5',\n",
    "                     49:'Gb:maj/3', 50:'Gb:min/b3', 51:'Gb:maj/5', 52:'Gb:min/5', 53:'G:maj/3', 54:'G:min/b3',\n",
    "                     55:'G:maj/5', 56:'G:min/5', 57:'Ab:maj/3', 58:'Ab:min/b3', 59:'Ab:maj/5', 60:'Ab:min/5',\n",
    "                     61:'A:maj/3', 62:'A:min/b3', 63:'A:maj/5', 64:'A:min/5', 65:'Bb:maj/3', 66:'Bb:min/b3',\n",
    "                     67:'Bb:maj/5', 68:'Bb:min/5', 69:'B:maj/3', 70:'B:min/b3', 71:'B:maj/5', 72:'B:min/5'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the right chromas and plug them into the network to get predictions.\n",
    "\n",
    "ChromasPerTimeStep = 24\n",
    "IdxToReadFrom = 100\n",
    "#Do batch_size batches of 50 time steps. 50 time steps is about 2s of audio.\n",
    "NumOfTimeStepsToRead = 50\n",
    "batch_size=5\n",
    "\n",
    "timeStampToReadFrom = 271\n",
    "IdxToReadFrom = 271*44100//2048\n",
    "\n",
    "net = prevNet\n",
    "\n",
    "chopinChromas = np.zeros((batch_size,NumOfTimeStepsToRead, ChromasPerTimeStep))\n",
    "\n",
    "with open('Data/ChopinChromagrams.csv', 'r') as f:\n",
    "    rdr = csv.reader(f)\n",
    "    batch_num=0\n",
    "    for idx, row in enumerate(rdr):\n",
    "        if IdxToReadFrom <= idx < IdxToReadFrom+NumOfTimeStepsToRead:\n",
    "\n",
    "            chopinChromas[batch_num,idx-IdxToReadFrom,:] = row[1:]\n",
    "            \n",
    "            if idx == IdxToReadFrom+NumOfTimeStepsToRead-1:\n",
    "                batch_num+=1\n",
    "                IdxToReadFrom = IdxToReadFrom+NumOfTimeStepsToRead\n",
    "\n",
    "            if batch_num==batch_size:\n",
    "                break\n",
    "\n",
    "chopinChromas = torch.Tensor(chopinChromas).double().to(device)\n",
    "\n",
    "#Input into the net and get them back on the CPU.\n",
    "chopin_predictions = net(chopinChromas).to(cpuDevice)\n",
    "chopin_predictions = chopin_predictions.view(batch_size*NumOfTimeStepsToRead,num_of_chord_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.0 Chord: F:maj\n",
      "Time:  0.0 Chord: N\n",
      "Time:  0.2 Chord: Bb:maj\n",
      "Time:  2.3 Chord: N\n",
      "Time:  3.0 Chord: Bb:maj\n",
      "Time:  4.6 Chord: N\n",
      "Time:  5.0 Chord: E:maj\n",
      "Time:  5.2 Chord: N\n",
      "Time:  5.9 Chord: A:maj\n",
      "Time:  7.0 Chord: N\n",
      "Time:  7.2 Chord: D:maj\n",
      "Time:  7.6 Chord: N\n",
      "Time:  8.3 Chord: C:maj\n",
      "Time:  8.5 Chord: C:min\n",
      "Time:  8.5 Chord: C:maj\n",
      "Time:  8.6 Chord: C:min\n",
      "Time:  9.1 Chord: Bb:maj\n",
      "Time:  9.3 Chord: N\n",
      "Time:  9.4 Chord: Bb:maj\n",
      "Time:  9.8 Chord: F:maj\n",
      "Time:  9.9 Chord: N\n",
      "Time: 10.0 Chord: F:maj\n",
      "Time: 10.1 Chord: N\n",
      "Time: 10.8 Chord: Bb:maj\n"
     ]
    }
   ],
   "source": [
    "#Get output and turn it back into chords.\n",
    "#chopin_predictions = (batch_num, time_step, chord_values)\n",
    "secondsPerTimeStep = 2048/44100\n",
    "chopin_chord_numbers = np.zeros((chopin_predictions.shape[0]))\n",
    "timesAndChords = []\n",
    "\n",
    "prevChord = -1\n",
    "\n",
    "for time_step in range(NumOfTimeStepsToRead*batch_size):\n",
    "    \n",
    "    #Which chord has the highest score.\n",
    "    maxIdx = torch.argmax(chopin_predictions[time_step]).item()\n",
    "    \n",
    "    #Add this chord number to an array to be median filtered.\n",
    "    chopin_chord_numbers[time_step] = maxIdx\n",
    "\n",
    "    #Work out the entropy in this decision\n",
    "    entropy = torch.distributions.Categorical(probs=chopin_predictions[time_step]).entropy().item()\n",
    "    \n",
    "    #Get the score of the chord out\n",
    "    chordScore = torch.max(chopin_predictions[time_step]).item()\n",
    "    \n",
    "    #Get the chord symbol out\n",
    "    chord = chord_dict_reverse[maxIdx]\n",
    "    if chord != prevChord:\n",
    "        #New chord onset!\n",
    "        timesAndChords += [[time_step*secondsPerTimeStep, chord]]\n",
    "    \n",
    "        prevChord = chord\n",
    "    \n",
    "    #print('Time:', time_step*secondsPerTimeStep, 'Chord:', chord, 'Score:', chordScore, 'Entropy:', entropy)\n",
    "for time, chord in timesAndChords:\n",
    "    print(\"Time: %4.1f\" % (time), 'Chord:', chord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesAndChords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.0 Chord: N\n",
      "Time:  0.2 Chord: F:maj\n",
      "Time:  0.2 Chord: Bb:maj\n",
      "Time:  2.3 Chord: N\n",
      "Time:  3.0 Chord: Bb:maj\n",
      "Time:  4.6 Chord: N\n",
      "Time:  5.9 Chord: A:maj\n",
      "Time:  7.0 Chord: D:maj\n",
      "Time:  7.6 Chord: N\n",
      "Time:  8.3 Chord: C:maj\n",
      "Time:  8.5 Chord: C:min\n",
      "Time:  9.2 Chord: Bb:maj\n",
      "Time:  9.8 Chord: F:maj\n",
      "Time: 10.0 Chord: N\n",
      "Time: 10.8 Chord: Bb:maj\n"
     ]
    }
   ],
   "source": [
    "#Median filter the chors and display nicely\n",
    "chopin_chord_numbers_filtered = medfilt(np.squeeze(chopin_chord_numbers), kernel_size=9)\n",
    "prevChord = -1\n",
    "filteredTimesAndChords = []\n",
    "for i, chordNumber in enumerate(chopin_chord_numbers_filtered):\n",
    "    if chordNumber != prevChord:\n",
    "        filteredTimesAndChords += [[i*secondsPerTimeStep, chord_dict_reverse[chordNumber]]]\n",
    "    prevChord=chordNumber\n",
    "    \n",
    "for time, chord in filteredTimesAndChords:\n",
    "    print(\"Time: %4.1f\" % (time), 'Chord:', chord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
